{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKt5sLQO9zgn"
   },
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Jo434asKrS0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfbM76wz9_kg"
   },
   "source": [
    "# The Forward Forward algorithm\n",
    "\n",
    "The [Forward Forward algorithm](https://www.cs.toronto.edu/~hinton/FFA13.pdf) has been presented by Geoffrey Hinton (one of the \"Deep Learning godfathers\") at NeurIPS 2022, less than a month ago.\n",
    "\n",
    "It is a novel technique for Neural Network optimization, alternative to standard Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztmvsf2WB94X"
   },
   "source": [
    "## An alternative to Backpropagation\n",
    "\n",
    "Over the last decades, backprop has been so successful that neuroscientists have been looking for proofs that the brain actually learns in a similar way. However, that does not seem to happen. Moreover, backprop encounters serious limitations if the model is not exactly defined in mathematical terms: if the forward pass is a black box, there is no possibility to make a backward pass (as we don't know what to differentiate).\n",
    "\n",
    "Even if (for the moment) it was not presented with the goal of substituting backprop for practical applications, the Forward Forward algorithm aims at solving some of these problems, using a layer-by-layer learning approach that does not require backward propagation of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-42jTD10GapS"
   },
   "source": [
    "## MNIST classification through FF\n",
    "\n",
    "Let's see how FF works through a practical example on MNIST classification.\n",
    "\n",
    "Citing the paper,  \"the idea is to replace the forward and backward passes of backpropagation by two forward\n",
    "passes that operate in exactly the same way as each other, but on different data and with opposite\n",
    "objectives. The positive pass operates on real data and adjusts the weights to increase the goodness in\n",
    "every hidden layer. The negative pass operates on \"negative data\" and adjusts the weights to decrease\n",
    "the goodness in every hidden layer. This paper explores two different measures of goodness â€“ the\n",
    "sum of the squared neural activities and the negative sum of the squared activities, but many other\n",
    "measures are possible.\"\n",
    "\n",
    "Thus, we have to find a way to define \"positive\" and \"negative\" data. If we want to classify MNIST, a data point being positive means that it somehow contains the right assignment of an image to its corresponding class. Hence, we can define a positive datapoint by embedding the right classification into the image and a negative datapoint by embedding a random classification. To do so, we can exploit the fact that there is a black frame around the digit, and we can use the top 10 pixels on the left to embed class information.\n",
    "\n",
    "![](images/ff_mnist.png)\n",
    "\n",
    "Then, the layers of the network are trained one at a time with the objective of assigning high activation values (high goodness) to positive points and low to negative ones. However, if there are many layers, it would be trivial for following layers to rely on the information coming from the previous ones (if the activations of layer $i$ are high, this drastically increases the probability that the activations of layer $i+1$ are high as well). To avoid this phenomenon and make layers learn different features, it is necessary to normalize the activation vector to have norm $1$.\n",
    "\n",
    "At the end, at evaluation time, one can query the network with a certain image with all possible labels, and take the argmax of the goodnesses to obtain the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zeyKi7Sj1gcn"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([ReLULayer(dimensions[i], dimensions[i + 1]) for i in range(len(dimensions)-1)])\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            x_lab = label_images(x, label)\n",
    "            goodness = []\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x_lab = layer(x_lab)\n",
    "                if i>0:\n",
    "                    goodness.append( ##################\n",
    "                        torch.mean(torch.square(x_lab),1)\n",
    "                    )\n",
    "            goodness_per_label.append(sum(goodness).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return torch.argmax(goodness_per_label, dim=1)\n",
    "    \n",
    "    def train(self, x_pos, x_neg):\n",
    "        for layer in self.layers:\n",
    "            x_pos, x_neg = layer.train(x_pos, x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zhuC5SwH1iCz"
   },
   "outputs": [],
   "source": [
    "def normalize(x): ###########\n",
    "    return torch.nn.functional.normalize(x)\n",
    "\n",
    "class ReLULayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = normalize(x)\n",
    "        return self.relu(self.linear(x_direction))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in range(self.num_epochs):\n",
    "            positive_goodness = torch.mean(torch.square(self.forward(x_pos)),1) ####################\n",
    "            negative_goodness = torch.mean(torch.square(self.forward(x_neg)),1) ####################\n",
    "            l = torch.log(1 + torch.exp(torch.cat([\n",
    "                -positive_goodness + self.threshold,\n",
    "                negative_goodness - self.threshold]))).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1dSgF_m1jW6",
    "outputId": "dcc28ed3-e9c5-4d1e-d87f-ab2da690a529"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 5.93 GiB total capacity; 4.63 GiB already allocated; 96.12 MiB free; 4.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17694/2366238753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17694/4201520123.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mgoodness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mx_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     goodness.append( ##################\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17694/2454849659.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 5.93 GiB total capacity; 4.63 GiB already allocated; 96.12 MiB free; 4.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,)), torchvision.transforms.Lambda(torch.flatten)])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data/', transform=transform,  train=True, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('./data/', transform=transform, train=False, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False)\n",
    "\n",
    "def label_images(images, labels):\n",
    "    #this function takes shape (batch_size, 784) images and shape (batch_size) labels and returns label-embedded images, with shape (batch_size, 784)\n",
    "    max_value = torch.max(images)\n",
    "    images = images.reshape(-1,784).clone()\n",
    "    for i, image in enumerate(images):\n",
    "        label = labels if type(labels) == int else labels[i]\n",
    "        image[label] = max_value\n",
    "    return images\n",
    "\n",
    "net = Net([784, 500, 500]).to(device)\n",
    "x, y = next(iter(trainloader))\n",
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "x_pos = label_images(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "\n",
    "x_neg = label_images(x, y[rnd])\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print('Train accuracy:', net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(testloader))\n",
    "x_te=x_te.to(device)\n",
    "y_te=y_te.to(device)\n",
    "\n",
    "print('Test accuracy:', net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U3i-Zo3Y75"
   },
   "source": [
    "# Third assignment (deadline 8 January)\n",
    "\n",
    "\n",
    "\n",
    "1.   Read Hinton's [paper](https://www.cs.toronto.edu/~hinton/FFA13.pdf) about the Forward Forward algorithm. In particular, pay attention to the first 6/7 pages of it.\n",
    "2.   Fill the gaps in the notebook to reproduce the experiment described at page 5 of the paper.\n",
    "3.   Experiment with two of the following three points for future work reported in the paper and discuss your findings:\n",
    "    *   Can the positive and negative passes be very widely separated in time as they would be if\n",
    "the negative passes were done during sleep?\n",
    "    *   What is the best goodness function to use?\n",
    "    *   What is the best activation function to use? \n",
    "\n",
    "You can send your work as a jupyter notebook in any format you prefer (`ipynb`, `pdf` or `html`) to lore.basile@outlook.com by 23.59, 08/01/2023. Please name the file as `NameSurname.<format>`.\n",
    "\n",
    "For any doubt do not hesitate to reach out at alessio.ansuini@areasciencepark.it and lore.basile@outlook.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_dUmZiD1b8M"
   },
   "source": [
    "# Future work\n",
    "## Can the positive and negative passes be very widely separated in time as they would be if the negative passes were done during sleep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s0mAX8C1Rz7"
   },
   "outputs": [],
   "source": [
    "class ReLULayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = normalize(x)\n",
    "        return self.relu(self.linear(x_direction))\n",
    "\n",
    "    def train_pos(self, x_pos):\n",
    "        for i in range(self.num_epochs):\n",
    "            positive_goodness = torch.mean(torch.square(self.forward(x_pos)),1) ####################\n",
    "            l = torch.log(1 + torch.exp(\n",
    "                -positive_goodness + self.threshold,\n",
    "               )).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.forward(x_pos).detach()\n",
    "          \n",
    "    def train_neg(self, x_neg):\n",
    "        for i in range(self.num_epochs):\n",
    "            negative_goodness = torch.mean(torch.square(self.forward(x_neg)),1) ####################\n",
    "            l = torch.log(1 + torch.exp(              \n",
    "                negative_goodness - self.threshold)).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.forward(x_neg).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSsGh08H-QgV"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([ReLULayer(dimensions[i], dimensions[i + 1]) for i in range(len(dimensions)-1)])\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            x_lab = label_images(x, label)\n",
    "            goodness = []\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x_lab = layer(x_lab)\n",
    "                if i>0:\n",
    "                    goodness.append( ##################\n",
    "                        torch.mean(torch.square(x_lab),1)\n",
    "                    )\n",
    "            goodness_per_label.append(sum(goodness).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return torch.argmax(goodness_per_label, dim=1)\n",
    "    \n",
    "    def train_pos(self, x_pos):\n",
    "        for layer in self.layers:\n",
    "            x_pos = layer.train_pos(x_pos)\n",
    "        \n",
    "    def train_neg(self, x_neg):\n",
    "        for layer in self.layers:\n",
    "            x_neg = layer_neg.train(x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net([784, 500, 500]).to(device)\n",
    "x, y = next(iter(trainloader))\n",
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "x_pos = label_images(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "\n",
    "x_neg = label_images(x, y[rnd])\n",
    "net.train_pos(x_pos)\n",
    "\n",
    "print('Train accuracy only with positive data:', net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(testloader))\n",
    "x_te=x_te.to(device)\n",
    "y_te=y_te.to(device)\n",
    "\n",
    "print('Test accuracy only with positive data:', net.predict(x_te).eq(y_te).float().mean().item())\n",
    "\n",
    "net.train_neg(x_neg)\n",
    "\n",
    "print('Train accuracy after negative data:', net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(testloader))\n",
    "x_te=x_te.to(device)\n",
    "y_te=y_te.to(device)\n",
    "\n",
    "print('Test accuracy after negative data:', net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
