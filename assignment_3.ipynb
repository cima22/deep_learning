{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKt5sLQO9zgn"
      },
      "source": [
        "# Lab 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "0Jo434asKrS0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfbM76wz9_kg"
      },
      "source": [
        "# The Forward Forward algorithm\n",
        "\n",
        "The [Forward Forward algorithm](https://www.cs.toronto.edu/~hinton/FFA13.pdf) has been presented by Geoffrey Hinton (one of the \"Deep Learning godfathers\") at NeurIPS 2022, less than a month ago.\n",
        "\n",
        "It is a novel technique for Neural Network optimization, alternative to standard Backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztmvsf2WB94X"
      },
      "source": [
        "## An alternative to Backpropagation\n",
        "\n",
        "Over the last decades, backprop has been so successful that neuroscientists have been looking for proofs that the brain actually learns in a similar way. However, that does not seem to happen. Moreover, backprop encounters serious limitations if the model is not exactly defined in mathematical terms: if the forward pass is a black box, there is no possibility to make a backward pass (as we don't know what to differentiate).\n",
        "\n",
        "Even if (for the moment) it was not presented with the goal of substituting backprop for practical applications, the Forward Forward algorithm aims at solving some of these problems, using a layer-by-layer learning approach that does not require backward propagation of gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-42jTD10GapS"
      },
      "source": [
        "## MNIST classification through FF\n",
        "\n",
        "Let's see how FF works through a practical example on MNIST classification.\n",
        "\n",
        "Citing the paper,  \"the idea is to replace the forward and backward passes of backpropagation by two forward\n",
        "passes that operate in exactly the same way as each other, but on different data and with opposite\n",
        "objectives. The positive pass operates on real data and adjusts the weights to increase the goodness in\n",
        "every hidden layer. The negative pass operates on \"negative data\" and adjusts the weights to decrease\n",
        "the goodness in every hidden layer. This paper explores two different measures of goodness â€“ the\n",
        "sum of the squared neural activities and the negative sum of the squared activities, but many other\n",
        "measures are possible.\"\n",
        "\n",
        "Thus, we have to find a way to define \"positive\" and \"negative\" data. If we want to classify MNIST, a data point being positive means that it somehow contains the right assignment of an image to its corresponding class. Hence, we can define a positive datapoint by embedding the right classification into the image and a negative datapoint by embedding a random classification. To do so, we can exploit the fact that there is a black frame around the digit, and we can use the top 10 pixels on the left to embed class information.\n",
        "\n",
        "![](images/ff_mnist.png)\n",
        "\n",
        "Then, the layers of the network are trained one at a time with the objective of assigning high activation values (high goodness) to positive points and low to negative ones. However, if there are many layers, it would be trivial for following layers to rely on the information coming from the previous ones (if the activations of layer $i$ are high, this drastically increases the probability that the activations of layer $i+1$ are high as well). To avoid this phenomenon and make layers learn different features, it is necessary to normalize the activation vector to have norm $1$.\n",
        "\n",
        "At the end, at evaluation time, one can query the network with a certain image with all possible labels, and take the argmax of the goodnesses to obtain the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "zeyKi7Sj1gcn"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dimensions):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList([ReLULayer(dimensions[i], dimensions[i + 1]) for i in range(len(dimensions)-1)])\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            x_lab = label_images(x, label)\n",
        "            goodness = []\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                x_lab = layer(x_lab)\n",
        "                if i>0:\n",
        "                    goodness.append( ##################\n",
        "                        torch.sum(torch.square(x_lab),dim=1)\n",
        "                    )\n",
        "            goodness_per_label.append(sum(goodness).unsqueeze(1))\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return torch.argmax(goodness_per_label, dim=1)\n",
        "    \n",
        "    def train(self, x_pos, x_neg):\n",
        "        for layer in self.layers:\n",
        "            x_pos, x_neg = layer.train(x_pos, x_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "zhuC5SwH1iCz"
      },
      "outputs": [],
      "source": [
        "def normalize(x): ###########\n",
        "    return torch.nn.functional.normalize(x)\n",
        "\n",
        "class ReLULayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(in_features, out_features)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.03)\n",
        "        self.threshold = 2.0\n",
        "        self.num_epochs = 1000\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = normalize(x)\n",
        "        return self.relu(self.linear(x_direction))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            positive_goodness = torch.sum(torch.square(self.forward(x_pos)),dim=1) ####################\n",
        "            negative_goodness = -torch.sum(torch.square(self.forward(x_neg)),dim=1)####################\n",
        "            l = torch.log(1 + torch.exp(torch.cat([\n",
        "                -positive_goodness + self.threshold,\n",
        "                negative_goodness - self.threshold]))).mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            l.backward()\n",
        "            self.optimizer.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1dSgF_m1jW6",
        "outputId": "d76ddd35-b86e-429c-b955-ac665a1df3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.0026666666381061077\n",
            "Test accuracy: 0.002099999925121665\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,)), torchvision.transforms.Lambda(torch.flatten)])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST('./data/', transform=transform,  train=True, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST('./data/', transform=transform, train=False, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False)\n",
        "\n",
        "def label_images(images, labels):\n",
        "    #this function takes shape (batch_size, 784) images and shape (batch_size) labels and returns label-embedded images, with shape (batch_size, 784)\n",
        "    max_value = torch.max(images)\n",
        "    images = images.reshape(-1,784).clone()\n",
        "    for i, image in enumerate(images):\n",
        "        label = labels if type(labels) == int else labels[i]\n",
        "        image[label] = max_value\n",
        "    return images\n",
        "\n",
        "net = Net([784, 500, 500]).to(device)\n",
        "x, y = next(iter(trainloader))\n",
        "x=x.to(device)\n",
        "y=y.to(device)\n",
        "x_pos = label_images(x, y)\n",
        "rnd = torch.randperm(x.size(0))\n",
        "\n",
        "x_neg = label_images(x, y[rnd])\n",
        "net.train(x_pos, x_neg)\n",
        "\n",
        "print('Train accuracy:', net.predict(x).eq(y).float().mean().item())\n",
        "\n",
        "x_te, y_te = next(iter(testloader))\n",
        "x_te=x_te.to(device)\n",
        "y_te=y_te.to(device)\n",
        "\n",
        "print('Test accuracy:', net.predict(x_te).eq(y_te).float().mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(trainloader))"
      ],
      "metadata": {
        "id": "QCkVXIZjK3JE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(input, output):\n",
        "    if input is not None:\n",
        "        input_pics = input.data.cpu().numpy().reshape(28,28)\n",
        "        plt.imshow(input_pics,cmap='gray')\n",
        "    plt.figure(figsize=(18, 4))\n",
        "    output_pics = output.data.cpu().numpy().reshape(28,28)\n",
        "    plt.imshow(output_pics,cmap='gray')\n",
        "\n",
        "index = 3\n",
        "number = 3\n",
        "display_images(x[index].reshape(-1,784),label_images(x[index].reshape(-1,784),number))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "W1sOcO4dHhcl",
        "outputId": "35a1c107-5b30-4359-f822-8fbb30ff6845"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ReLULayer(784,500)\n",
        "x = torch.rand(4,784)"
      ],
      "metadata": {
        "id": "1qv0rOARragD"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(torch.square(model.forward(x)),dim=1))\n",
        "print(torch.sum(torch.square(model(x)),dim=1) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lThGCbZ_sCyT",
        "outputId": "da205551-f188-4105-e134-ecae0a15628c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2283, 0.2297, 0.2190, 0.2363], grad_fn=<SumBackward1>)\n",
            "tensor([0.2283, 0.2297, 0.2190, 0.2363], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28U3i-Zo3Y75"
      },
      "source": [
        "# Third assignment (deadline 8 January)\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Hinton's [paper](https://www.cs.toronto.edu/~hinton/FFA13.pdf) about the Forward Forward algorithm. In particular, pay attention to the first 6/7 pages of it.\n",
        "2.   Fill the gaps in the notebook to reproduce the experiment described at page 5 of the paper.\n",
        "3.   Experiment with two of the following three points for future work reported in the paper and discuss your findings:\n",
        "    *   Can the positive and negative passes be very widely separated in time as they would be if\n",
        "the negative passes were done during sleep?\n",
        "    *   What is the best goodness function to use?\n",
        "    *   What is the best activation function to use? \n",
        "\n",
        "You can send your work as a jupyter notebook in any format you prefer (`ipynb`, `pdf` or `html`) to lore.basile@outlook.com by 23.59, 08/01/2023. Please name the file as `NameSurname.<format>`.\n",
        "\n",
        "For any doubt do not hesitate to reach out at alessio.ansuini@areasciencepark.it and lore.basile@outlook.com.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}