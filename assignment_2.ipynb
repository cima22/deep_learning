{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f39c36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dadapy import data\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import exists, isdir\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5865bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kmnist(data_path = \"kmnist\", force_download = True):\n",
    "    \"\"\"Function that fetches the train and test kmnist dataset and returns it in a numpy array.\n",
    "    It downloads it if the file is not present or if force_download is True.\"\"\"\n",
    "    \n",
    "    url = 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-'\n",
    "    train_url = url + 'train-imgs.npz'\n",
    "    test_url  = url + 'test-imgs.npz'\n",
    "    \n",
    "    train_path = data_path + \"/train_set\"\n",
    "    test_path  = data_path + \"/test_set\"\n",
    "    \n",
    "    if not isdir(data_path):\n",
    "        makedirs(data_path)\n",
    "    \n",
    "    if not exists(train_path) or force_download:\n",
    "        print(f\"downloading data from {train_url} to {train_path}\")\n",
    "        urlretrieve(train_url, train_path)\n",
    "    if not exists(test_path) or force_download:\n",
    "        print(f\"downloading data from {test_url} to {test_path}\")\n",
    "        urlretrieve(test_url, test_path)\n",
    "    \n",
    "    train_set = np.load(train_path)['arr_0']\n",
    "    test_set  = np.load(test_path)['arr_0']\n",
    "        \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c7f1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = fetch_kmnist(force_download = False)\n",
    "train_set_flat = train_set.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb3e10e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_img=train_set_flat[3454]\n",
    "_ = plt.imshow(first_img.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aad096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _data = data.Data(train_set_flat)\n",
    "# estimate ID\n",
    "# id_twoNN, _, r = _data.compute_id_2NN()\n",
    "id_twoNN = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2d08901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1, padding = 0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2),\n",
    "            torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2, padding = 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(6*6*64, id_twoNN)\n",
    "        )\n",
    "    \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(id_twoNN, 64*8*8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Unflatten(1, torch.Size([64, 8, 8])),\n",
    "            torch.nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 2, stride = 2, padding = 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels = 32, out_channels = 1, kernel_size = 2, stride = 2, padding = 0),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da14e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62129e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "loss = torch.nn.MSELoss(reduction = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0af33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz\"\n",
    "urlretrieve(url, \"kmnist/train_label\")\n",
    "train_labels = np.load(\"kmnist/train_label\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7bf3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "torch_train_set = TensorDataset(torch.Tensor(train_set)) # create your datset\n",
    "train_dataloader = DataLoader(torch_train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torch_test_set = TensorDataset(torch.Tensor(test_set)) # create your datset\n",
    "test_dataloader = DataLoader(torch_test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65cfbc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a0eeee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-b9ac8d231006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mx_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-f3d38d946c78>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x in iter(train_dataloader):\n",
    "        x_hat = model(x)\n",
    "        l = loss(x_hat,x)\n",
    "        train_loss += l.item()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch \"+ str(epoch+1) + \" Training loss:\" + str(train_loss / (len(train_dataloader.dataset))))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_loss=0 \n",
    "    for x in iter(test_dataloader):\n",
    "            x_hat=model(x)\n",
    "            l=loss(x_hat,x)\n",
    "            test_loss+=l.item()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print(\"Test set loss:\"+str(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
